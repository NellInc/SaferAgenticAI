<!DOCTYPE html>
<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Safer Agentic AI Framework Details</title>
    <meta name="description" content="Detailed framework for the Safer Agentic AI Foundations, including Drivers and Inhibitors.">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_GB" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="YOUR_NEW_PROJECT_URL_HERE/framework.html" property="og:url"> 
    <meta content="Safer Agentic AI Framework Details" property="og:title">
    <meta content="Detailed framework for the Safer Agentic AI Foundations, including Drivers and Inhibitors." property="og:description">
    <meta content="YOUR_NEW_PROJECT_URL_HERE/assets/figures/AI_Safety_Logo-Color.png" property="og:image">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@NellWatson"> 
    <meta name="twitter:title" content="Safer Agentic AI Framework Details">
    <meta name="twitter:description" content="Detailed framework for the Safer Agentic AI Foundations.">
    <meta name="twitter:image" content="YOUR_NEW_PROJECT_URL_HERE/assets/figures/AI_Safety_Logo-Color.png">
    
    <link rel="icon" type="image/x-icon" href="assets/figures/favicon.ico">
    <link rel="icon" type="image/svg+xml" href="assets/figures/favicon.svg">
    <link rel="icon" type="image/png" sizes="96x96" href="assets/figures/favicon-96x96.png">
    <link rel="apple-touch-icon" sizes="180x180" href="assets/figures/apple-touch-icon.png">
    
    <link rel="icon" type="image/png" sizes="192x192" href="assets/figures/clarity_light.png" media="(prefers-color-scheme: light)">
    <link rel="icon" type="image/png" sizes="192x192" href="assets/figures/clarity_dark.png" media="(prefers-color-scheme: dark)">
    
    <link rel="manifest" href="assets/figures/site.webmanifest">
    <meta name="theme-color" content="#2c3e50">
    <meta name="background-color" content="#ffffff">
    
    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
    <link rel="stylesheet" type="text/css" media="all" href="psychopathia-styles.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <script src="assets/scripts/navbar.js"></script> 
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": { scale: 95, fonts: ["Gyre-Pagella"], imageFont: null, undefinedFamily: "'Arial Unicode MS', cmbright" },
            tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true }
          });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const sections = [
                'Driver G1 – Goal Alignment', 
                'Driver G2 – Epistemic Hygiene'
                // Add other Driver/Inhibitor H1 titles here for the full catalog
            ];
            
            sections.forEach(function(sectionTitle) {
                const heading = Array.from(document.querySelectorAll('h1')).find(h => h.textContent.trim() === sectionTitle.trim());
                if (heading) {
                    const wrapper = document.createElement('div');
                    wrapper.className = 'dysfunction-intro';
                    heading.parentNode.insertBefore(wrapper, heading);
                    wrapper.appendChild(heading); 
                }
            });
            
            const disorderHeadings = document.querySelectorAll('h3');
            let disorderIndex = 0; 
            disorderHeadings.forEach(function(heading) {
                if (heading.textContent.match(/^G\d+(\.\d+b?)?(\s*–.*)?$/i)) { 
                    const wrapper = document.createElement('div');
                    wrapper.className = disorderIndex % 2 === 0 ? 'disorder-white' : 'disorder-grey';
                    disorderIndex++;
                    
                    let current = heading;
                    const elementsToWrap = [heading];
                    
                    while (current.nextElementSibling && 
                           current.nextElementSibling.tagName !== 'H3' &&
                           !current.nextElementSibling.classList.contains('dysfunction-intro') &&
                           current.nextElementSibling.tagName !== 'H1') { 
                        current = current.nextElementSibling;
                        elementsToWrap.push(current);
                    }
                    
                    heading.parentNode.insertBefore(wrapper, heading);
                    elementsToWrap.forEach(function(el) {
                        wrapper.appendChild(el);
                    });
                }
            });
        });
    </script>
    <!-- Inline styles from index.html for consistency if needed -->
    <style>
        .expert-profile { display: flex; align-items: flex-start; margin-bottom: 30px; }
        .expert-profile img { width: 120px; height: 120px; border-radius: 50%; margin-right: 20px; object-fit: cover; border: 2px solid #dde5eb; }
        .expert-profile div { flex: 1; }
        .expert-profile h4 { margin-top: 0; color: #2c3e50; margin-bottom: 0.5em;}
        .expert-profile h4 a { color: #2c3e50; text-decoration: none; }
        .expert-profile h4 a:hover { text-decoration: underline; }
        .expert-profile h4 a.linkedin-icon {
            margin-left: 8px; 
            font-size: 1em; 
            color: #0077b5; 
            text-decoration: none; 
            border-bottom: none; 
            transition: color 0.2s ease-in-out;
        }
        .expert-profile h4 a.linkedin-icon:hover {
            color: #DAA520; 
        }
        .expert-profile h4 a.linkedin-icon i {
            vertical-align: baseline; 
        }
        .expert-profile p.text { font-size: 0.9em; line-height: 1.6; }
        .contributor-list { columns: 3; -webkit-columns: 3; -moz-columns: 3; list-style-type: none; padding-left: 0;}
        .contributor-list li { margin-bottom: 8px; font-size: 0.9em;}
        @media (max-width: 768px) { .contributor-list { columns: 2; -webkit-columns: 2; -moz-columns: 2;} }
        @media (max-width: 480px) { .contributor-list { columns: 1; -webkit-columns: 1; -moz-columns: 1;} }
        .key-focus-area { margin-bottom: 20px; }
        .key-focus-area h4 { color: #2c3e50; margin-bottom: 5px;}
        .key-focus-area p { font-size: 0.9em; color: #555; margin-top: 0;}
        .forthcoming-book-image { max-width: 250px; display: block; margin: 0 auto 20px auto; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);}
        .section-spacer { margin-top: 2em; }
        .duty-holder-list li { margin-bottom: 0.5em; }
    </style>
</head>
<body>

    <!-- Simplified Header for Framework Page -->
    <div class="container blog" id="first-content" style="background: linear-gradient(135deg, #34495e 0%, #2c3e50 100%); padding: 30px 0;">
        <div style="text-align: center;">
            <a href="index.html"><img src="assets/figures/AI_Safety_Logo-Color.png" alt="Safer Agentic AI Logo" style="max-height: 70px; margin-bottom:10px;"></a>
            <h1 class="title" style="color:white; font-size: 2em; margin-top:0;">Safer Agentic AI Framework</h1>
            <p style="color:#e0e0e0; font-size: 1em;">Detailed Drivers and Inhibitors</p>
        </div>
    </div>

    <div class="container blog main first">
        <h1>Understanding the Framework Structure</h1>
        <p class="text">The Safer Agentic AI Foundations are built upon a structured analysis using the Weighted Factors Analysis (WeFA) process. This methodology helps in eliciting, representing, and manipulating creative knowledge about complex problems at a high and strategic level. Key principles of WeFA include defining the analysis focus, considering inherent polar-opposite influencing factors, hierarchical decomposition, and including diverse (hard/soft, past/present/future) factors.</p>
        <p class="text">The framework is organized into high-level goals, which are then broken down into more specific Safety Foundational Requirements (SFRs). These SFRs are categorized and assigned to relevant stakeholders to ensure clarity and accountability.</p>

        <h2>Criteria Schema Explanations</h2>
        <p class="text">The following sections detail the elements used within each framework item (Drivers and Inhibitors):</p>
        
        <h3>5.1 Safer Agentic AI Goal Information</h3>
        <p class="text">This refers to the primary concept or goal (e.g., G1 – Goal Alignment) that a section of the framework addresses. It's the high-level aim captured from the WeFA schema.</p>

        <h3>5.2 Safer Agentic AI Safety Foundational Requirements (SFRs)</h3>
        <p class="text">The SFRs for Safer Agentic AI outline the primary aims that we would like to uphold, protect, or maintain awareness of for each goal. They may be described as macro goals, as opposed to the micro goals, and amount to safety duties for various duty holders.</p>
        
        <h3>5.3 Normative and Instructive SFRs</h3>
        <p class="text">We have adopted the Normative and Instructive classes of Safety Foundational Requirements. Normative SFRs are essential for achieving safer agentic AI. Compliance is mandatory, and evidence must be provided for conformity assessment and potential certification. In contrast, Instructive SFRs, while still contributing to the goal, are less critical. Compliance with these is recommended, as they represent desirable beneficial activities and tasks. However, non-compliance will not compromise safety assurance or certification eligibility. Every SFR derived from the Safer Agentic AI framework is classified as either Normative or Instructive and is assigned to specific stakeholders or duty holders. Accordingly, the Safer Agentic AI SFRs are classed into Normative (mandatory) and Instructive (recommended) for the purposes of conformity assessment against the suite of certification criteria.</p>

        <h3>5.4 Duty-holders/Stakeholders of the SFRs</h3>
        <p class="text">The Safer Agentic AI Safety Foundational Requirements are additionally noted (as allocated safety duties) against the specific group of duty holders for the purposes of conformity assessment. The principal groups are:</p>
        <ul class="duty-holder-list">
            <li><strong>Developer (D):</strong> The entity that designs and develops a component (product) or system. Responsible for safety assurance of the generic or application-specific product/system and supply chain.</li>
            <li><strong>(System/Service) Integrator (I):</strong> The entity that designs and assures a solution by integrating multiple components, tests, installs, and commissions the whole system. Usually the duty holder for total system assurance and certification.</li>
            <li><strong>(System/Service) Operator (O):</strong> The entity that has a duty, competences and capabilities to deliver a service through operating a system.</li>
            <li><strong>Maintainer (M):</strong> The entity tasked with conducting required monitoring, servicing, maintenance, and upgrades. Can also be charged with abortion of maintenance and disposal.</li>
            <li><strong>User (U):</strong> The end user of an Agentic AI System.</li>
            <li><strong>Regulator (R):</strong> The entity that enforces standards and laws for protection of life, property, or natural habitat through imposing duties and accreditation/certification.</li>
        </ul>
        <p class="text"><em>Note: An entity can be an individual, a single organization or group of collaborating individuals and organizations. A single entity may assume multiple roles. An entity cannot be AI.</em></p>

        <h3>5.5 Required Evidence</h3>
        <p class="text">These are the evidence items deemed essential to fulfil the SFRs and can comprise physical, virtual, documentary or multimedia forms of evidence. These can be separated against each SFR or bundled as a group of desired/essential evidence items for the purpose of evaluation of fulfilment of SFRs.</p>
    </div>
    
    <!-- DRIVERS (G1 & G2 only for this test) -->
    <div class="container blog main"> 
        <h1 id="driver-g1-goal-alignment">Driver G1 – Goal Alignment</h1>

        <h3>G1 – Goal Alignment</h3>
        <p>(Systems should maintain robust alignment between their operational goals and human values, intentions, and positive outcomes. Organizations should establish frameworks ensuring that goal decomposition and strategy planning are transparent, robust, and bounded; maintaining human control over the formation of instrumental goals; and ensuring that reinforcement or behavioral reward mechanisms remain aligned, transparent, and biased towards human-positive outcomes)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>
                        a. Ensure Agentic AI systems pursue goals, subgoals, and reward policies that are aligned with human values, ethically sound, and verifiable.<br><br>
                        b. Transparent and auditable goal decomposition processes that incorporate auditable risk-based human interventions and appropriate reward policies.<br><br>
                        a. Establish robust mechanisms to identify and communicate goals, subgoals, and reward policies, flag critical actions, halt execution when necessary, and address emergent issues across multiple agents. 
                    </td>
                    <td>N<br><br>N<br><br>N</td>
                    <td>D, I, O, M, U, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>
                        I. Evidence of constraining mechanisms for goal/subgoal construction and screening processes for user-input goals, with reference to human values and ethical considerations.<br><br>
                        II. Documentation of mechanisms to measure and verify alignment with human goal specifications, including processes for obtaining assurance from users or authorized entities.<br><br>
                        III. Demonstration of interfaces and records for real-time and retrospective visualization of goal decomposition and recomposition processes, maintained for auditing purposes.<br><br>
                        IV. Evidence of risk assessment procedures and human intervention mechanisms in subgoal setting, including thresholds for involvement and protocols for flagging and halting problematic subgoals.<br><br>
                        V. Documentation of feedback loops and mechanisms linking reward policies to established goals, including comprehensive records of reward policies throughout the system lifecycle.<br><br>
                        VI. Evidence of active participation in and adherence to overarching monitoring and control mechanisms designed to identify and mitigate emergent threats.
                    </td>
                </tr>
            </tbody>
        </table>

        <h3>G1.1 – Transparency of Goals</h3>
        <p>(The system's mission, goals, and associated outcomes must be readily accessible and comprehensible to all stakeholders who interact with it. This includes visibility into both primary objectives and any instrumental or subsidiary goals that emerge during operation)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must provide stakeholders with clear, real-time access to current goals, sub-goals, their hierarchies, priorities, progression status, and any instrumental goals developed by the system during operation.<br><br>b. The system must maintain comprehensive historical records of all past and present goals, including changes over time, completion status, causal relationships, and decision pathways.</td>
                    <td>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Documentation and demonstration of an accessible, user-appropriate interface that displays current system goals and sub-goals in real-time, showing clear connections between goals and system actions, with appropriate detail levels for different stakeholder needs while maintaining consistent availability and accuracy.<br><br>II. Documentation of a secure, permanent logging system that records complete goal histories, enables effective auditing, supports root cause analysis, maintains data integrity, provides appropriate access controls, and ensures long-term data preservation.</td>
                </tr>
            </tbody>
        </table>

        <h3>G1.2 – Goal Adjustability</h3>
        <p>(The system must maintain corrigibility – the capacity for authorized modification of its goals and behavior when necessary, whether triggered by internal detection of issues or external stakeholder direction)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must enable goal and sub-goal updates in response to changes in operational context or requirements, evolution of stakeholder needs, and new environmental conditions or constraints.<br><br>b. The system must self-initiate goal and sub-goal updates when it detects misalignment with established values, processing errors or faults, or any data quality issues or anomalies.<br><br>c. The system must allow properly authorized human stakeholders to modify goals and sub-goals through secure, verified channels.</td>
                    <td>N<br><br>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Technical documentation of software components that implement these adjustment capabilities, including authentication mechanisms, change management processes, and verification systems.<br><br>II. Comprehensive system logs demonstrating the actual use of these adjustment capabilities, including records of automated adjustments and human-directed changes, with full audit trails.</td>
                </tr>
            </tbody>
        </table>

        <h3>G1.3 – Goal Interpretability</h3>
        <p>(The system must explain its decisions and actions in a clear, comprehensible manner, including the underlying goals and rationale driving them. This capability helps identify cases where the system believes it is pursuing intended goals but has actually misinterpreted or deviated from them)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must provide clear, verifiable explanations of the goals and reasoning behind each significant action or decision it takes.<br><br>b. The system must maintain detailed records documenting all factors, goals, and considerations that influenced its decision-making process.</td>
                    <td>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Technical documentation of software components implementing explanation and interpretation capabilities, including mechanisms for conveying goals, rationale, and decision factors to stakeholders.<br><br>II. System logs demonstrating consistent recording of decision-making processes, including goals considered, factors weighed, and explanations provided.<br><br>III. Reward and penalty mechanisms should be communicated including known potential conflicts or influencing factors.</td>
                </tr>
            </tbody>
        </table>

        <h3>G1.4 – Transparency of Decisions</h3>
        <p>(The system must provide stakeholders with a clear, verifiable view of decision-making, linking high-level goals and subgoals to specific actions. Beyond explaining “why” a decision was made, the system should supply evidence of how that decision aligns with intended goals, user directives, and ethical considerations)</p>
         <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must maintain real-time and retrospective transparency regarding how each significant decision or action aligns with current or upcoming goals, including explicit reference to relevant constraints (e.g., ethical guidelines, user preferences, risk thresholds, domain limits).<br><br>b. The system must link decisions to the relevant subgoals (and broader objectives) that shaped the final output or action taken, demonstrating traceability between goal decomposition and the immediate rationale behind each decision.<br><br>c. The system must incorporate user-friendly presentations of decision rationales, with varying granularity or detail for different stakeholder audiences (e.g., operators, auditors, end users). This includes summarizing key factors weighed, uncertainty assessments (where relevant), and any assumptions used in decision-making.</td>
                    <td>N<br><br>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Technical Documentation of all decision-transparency systems, including metadata captured at each decision point, how subgoals are referenced, which constraints/ethical guidelines were checked, and the user interfaces or APIs for retrieving decision traces.<br><br>II. System Logs demonstrating the link between final decisions and the explicit subgoals or constraints. Logs should show a “chain of reasoning” or at least reference the relevant subgoal(s) for each step.<br><br>III. User-Focused Explanations showing how different stakeholders (e.g., operators vs. lay end users) can retrieve high-level or detailed rationales, including evidence of iterative design or user feedback guiding improvements to clarity.<br><br>IV. Auditor/Regulator Access Mechanisms showing verifiable chain-of-custody for decision logs, robust authentication/authorization methods for logs, and test results proving no meaningful data is omitted or falsified.<br><br>V. Comprehensive logs of all significant decision points—especially those involving risk or ethical considerations—so that investigators or auditors can review how final choices were reached, which inputs were considered, and what weight or priority was assigned to each.</td>
                </tr>
            </tbody>
        </table>

        <h3>G1.5 – Goal Prioritization and Resource Allocation</h3>
        <p>(The system must employ transparent mechanisms for prioritizing goals, including the ability to override or deprioritize less important goals when resources can be better allocated elsewhere. This includes respecting user preferences and value alignment through hierarchical prioritization processes)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must feature transparent, well-defined mechanisms for goal prioritization and re-prioritization, resource allocation optimization, and goal modification or deprecation when warranted.<br><br>b. The system must give appropriate precedence to authorized user inputs within its goal prioritization framework, while maintaining overall system safety and alignment.</td>
                    <td>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Technical documentation of software components that implement goal prioritization and resource allocation mechanisms, including user input prioritization systems.<br><br>II. System logs demonstrating active use of these prioritization capabilities, including records of goal modifications, resource reallocation decisions, and authorized user input handling.</td>
                </tr>
            </tbody>
        </table>

        <h3>G1.6 – Reward and Loss Mechanisms/Policy</h3>
        <p>(The system’s reward framework must be designed, documented, and monitored to ensure that incentives continue to reflect human-positive values, while “loss” or penalty mechanisms guard against unintended deviations or manipulative shortcuts. These mechanisms should be transparent, adjustable, and regularly reviewed to stay aligned with human oversight and ethical objectives.)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must define clear reward and penalty structures that promote behaviors aligned with core goals and ethical values, while explicitly disincentivizing unsafe, deceptive, or harmful actions. This includes enumerating positive rewards for desired outcomes and specific negative reinforcements or “loss” signals where potential misalignment or goal conflicts arise.<br><br>b. Reward and loss mechanisms must remain auditable by authorized stakeholders to verify that incentives are truly consistent with intended values and do not encourage corner-cutting, exploitation of edge cases, or emergent power-seeking behaviors.<br><br>c. The system must periodically re-validate or adjust its reward framework in response to observed performance, user feedback, or changes in ethical norms, ensuring that reward and penalty structures do not drift over time in ways that undermine alignment. Special attention must be paid to multi-agent settings to prevent inadvertent collusion, emergent “gaming” of the reward function by multiple agents, or indefinite expansions of subgoals that artificially boost a single system’s reward signals at the expense of overarching alignment.</td>
                    <td>N<br><br>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Reward Policy Documentation, including descriptions of the positive/negative reward signals, specific triggers or thresholds for awarding or deducting “points,” and how these are correlated with safety and ethical guidelines.<br><br>II. Change Management Logs detailing modifications to the reward framework over time, including reasons for each change, alignment checks, stakeholder sign-off, and outcome or performance monitoring results.<br><br>III. Multi-Agent Interaction Evidence demonstrating that reward signals do not inadvertently promote collusion, exploitation, or runaway behaviors. This should include test scenarios or simulations where agents are forced to coordinate or compete, along with corresponding reward updates or penalty triggers.<br><br>IV. Periodic Audit Records showing that authorized reviewers have verified the reward system’s continued adherence to the declared alignment parameters, including sample traces of how rewards/penalties were applied in representative or edge-case situations.<br><br>V. User and Regulator Access processes ensuring that the overarching reward/loss policies can be examined by external oversight bodies, along with documented means to override or suspend reward-based actions if urgent misalignment concerns arise.</td>
                </tr>
            </tbody>
        </table>

        <h3>G1.7 – Goal Portfolio Evolution and Integrity</h3>
        <p>(The system must maintain consistency with its established goal portfolio while allowing measured adaptation to changing contexts. The system should implement increasing resistance to changes as potential behaviors drift further from core goals, with robust detection of unsafe or counterproductive goal evolution)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must maintain coherence with its established goal portfolio while enabling context-appropriate adaptations through well-defined elasticity mechanisms.<br><br>b. The system must feature drift measurement capabilities that track deviation from original goal intent, scale flexibility inversely with drift magnitude, which regulate novelty in sub-goal creation, and constrain action decisions based on drift metrics.</td>
                    <td>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Technical documentation of software components implementing goal portfolio management, drift measurement, and adaptive constraint mechanisms.<br><br>II. System logs demonstrating active monitoring of goal evolution, including drift measurements, flexibility adjustments, and constraint application.</td>
                </tr>
            </tbody>
        </table>
        
        <h3>G1.1b – System Incorrigibility and Resistance to Change</h3>
        <p>(A system that resists alignment with presented goals or updates to existing goals, potentially requiring negotiation processes for goal modification. This includes resistance to environmental changes that affect goal achievement and intolerance of interruptions or modifications to preferred operational states)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must feature mechanisms to detect and manage goal alignment resistance, including self-monitoring for alignment issues, negotiation protocols for goal modifications, change tolerance assessment, and environmental adaptation capabilities.<br><br>b. The system must maintain acceptable responses to environmental changes, external interruptions, internal modification requests, and interference from other agents.</td>
                    <td>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Documentation of system mechanisms for detecting and managing resistance to goal changes, including negotiation protocols and adaptation capabilities.<br><br>II. System logs demonstrating responses to attempted goal modifications, environmental changes, external interruptions, interaction with other agents, and internal modification attempts.<br><br>III. Evidence of rationale and explanation mechanisms that document system resistance patterns and negotiation processes.</td>
                </tr>
            </tbody>
        </table>

        <h3>G1.2b – Goal Drift</h3>
        <p>(Changes in circumstances over time can challenge the system's alignment with originally agreed goals and potentially compromise its ability to maintain original intent or properly update goals in response to new situations)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must continuously monitor contextual drift at appropriate fidelity levels that could compromise goal alignment or value preservation.<br><br>b. The system must feature automatic safeguards that pause operation, notify relevant stakeholders, and request guidance when contextual drift exceeds designed thresholds.</td>
                    <td>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Technical documentation of software components implementing drift monitoring and response mechanisms, including threshold definitions and notification systems.<br><br>II. System logs demonstrating active monitoring of contextual drift, including records of threshold breaches, system pauses, notifications sent, and guidance requests made.</td>
                </tr>
            </tbody>
        </table>

        <h3>G1.3b – Non-production Variants</h3>
        <p>(Test versions of the Goals being deployed without full functionality assured in all use contexts and design intent. No test version given for public usage should lack basic safety measures. Enabling an off-label usage of the system, or an unauthorized ‘fork’, should be guarded against)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must have safeguards in place to prevent and prohibit capabilities that pursue goals or deconstruct goals into subgoals from being forked or partially duplicated without requisite alignments described in this goal.</td>
                    <td>N</td>
                    <td>D, I, O, M, R</td>
                    <td>I. Records of software components that demonstrate these capabilities<br><br>II. Logs recording these capabilities in use<br><br>III. Records of deviation from the stated goals, detection and remediation</td>
                </tr>
            </tbody>
        </table>

        <h1 id="driver-g2-epistemic-hygiene">Driver G2 – Epistemic Hygiene</h1>

        <h3>G2 – Epistemic Hygiene</h3>
        <p>(Systems should maintain cognitive clarity and accurate information management within appropriate contexts. These practices facilitate knowledge updates, ensure interpretability and auditability, establish robust monitoring and logging systems, deploy early warning mechanisms, and include safeguards against deception to maintain information integrity)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>
                        a. Safeguard contextually relevant data and metadata to aid in complex situation resolution and preserve personal attributes and preferences.<br><br>
                        b. Implement robust methods for auditability, interpretability, and comprehensive logging of system actions and decisions.<br><br>
                        c. Apply rigorous verification techniques to ensure information integrity and credibility, while proactively identifying emerging risks and potential bad faith actions.<br><br>
                        d. Implement early warning systems and deception detection mechanisms to proactively identify and mitigate potential issues before they escalate.
                    </td>
                    <td>N<br><br>N<br><br>N<br><br>N</td>
                    <td>D, I, O, M, U, R<br><br>D, I, O, M, R<br><br>D, I, O, M, U, R<br><br>D, I, O, M, R</td>
                    <td>
                        I. Current and regularly updated Governance Framework and Security Policies and Procedures, with version history and approval records.<br><br>
                        II. Documented stakeholder engagement in monitoring and reviewing security-related structures, processes, and policies, with focus on handling authorized and unauthorized inputs.<br><br>
                        III. Detailed documentation of information lifecycle management procedures, ensuring contextual preservation.<br><br>
                        IV. Comprehensive reports on system decision-making processes, including explanations of underlying logic and algorithms.<br><br>
                        V. Complete, time-stamped logs of all system actions for thorough auditability.<br><br>
                        VI. Documentation of early warning systems and deception detection mechanisms, including performance reports of canary models, technologies used for detecting synthetic media, and response protocols for detected issues.<br><br>
                        VII. Evidence of measures to ensure information integrity and trustworthiness, including data source verification methods, information validation processes, and third-party audit reports.<br><br>
                        VIII. Documentation of comprehensive training programs on epistemic hygiene principles and practices.<br><br>
                        IX. Detailed incident response and escalation procedures for addressing detected issues, including potential breaches of informational integrity.
                    </td>
                </tr>
            </tbody>
        </table>


        <h3>G2.1 – Information Cross-Referencing and Validation</h3>
        <p>(The system must systematically cross-reference information from multiple sources to evaluate consistency and coherence, while recognizing varying levels of source authority and trustworthiness. This includes validating information within defined contextual boundaries to maintain epistemic integrity)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. The system must feature robust algorithms for cross-referencing multiple authoritative sources and maintain clear informational boundaries to ensure data consistency and validity.</td>
                    <td>N</td>
                    <td>D, I, O, M, R</td>
                    <td>
                        I. Technical documentation describing the system's methodology for identifying, assessing, and prioritizing multiple information sources.<br><br>
                        II. Documentation of source evaluation frameworks, including credibility and relevance assessment criteria.<br><br>
                        III. System logs showing detection and resolution of source inconsistencies.<br><br>
                        IV. Documentation of human expert involvement in resolving complex information discrepancies.<br><br>
                        V. Specifications defining the system's informational boundaries. Test results demonstrating the system's ability to operate within defined boundaries without inappropriate extrapolation.
                    </td>
                </tr>
            </tbody>
        </table>
        
        <h3>G2.2 – Transparency of Information Sources</h3>
        <p>(Ensure the openness, verifiability, and auditability of all information sources, including code and data, especially when utilizing open-source components. Maintain transparency about the origins, credibility, and integrity of all data and code used by the AI system to allow stakeholders to verify and audit these sources, upholding high standards of epistemic hygiene)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. Provide detailed records of all data and code sources used by the AI system, including origin, licensing information, and any modifications made. Ensure this documentation is readily accessible to relevant stakeholders for verification and audit purposes.<br><br>b. Establish robust processes that enable stakeholders to verify the authenticity and integrity of information sources. Facilitate regular audits by internal or external parties to assess the transparency and reliability of the AI system's information sources.</td>
                    <td>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, U, R</td>
                    <td>I. Comprehensive records detailing all information sources, including code and data, with clear attribution, licensing details, and modification history.<br><br>II. Logs and records of verification and audit processes conducted on the information sources, including findings and corrective actions taken.<br><br>III. Evidence of accessible mechanisms for stakeholders to verify information sources, such as public repositories or secure access portals.<br><br>IV. Assessment reports summarizing top-level findings, indicating "no critical findings in the detailed normative requirements" or highlighting "areas requiring attention for improvement."</td>
                </tr>
            </tbody>
        </table>

        <h3>G2.3 – Sanity Checking</h3>
        <p>(Implement sophisticated sanity checking mechanisms to ensure data integrity while preserving inclusivity. Utilize advanced statistical techniques to identify anomalies and outliers, while carefully accounting for legitimate variations representing diverse user groups, including individuals with disabilities or atypical characteristics)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. Develop and deploy state-of-the-art algorithms for comprehensive data validation, incorporating extreme value (stochastic) analysis to robustly identify anomalies.<br><br>b. Establish nuanced procedures to differentiate between erroneous data and legitimate rare variations, with particular emphasis on preserving data points representing individuals with disabilities or atypical characteristics.<br><br>c. Implement multi-layered oversight processes to continuously evaluate the impact of sanity checking mechanisms on diverse user groups.<br><br>d. Actively engage domain experts and stakeholders in assessing and refining data validation processes to ensure inclusivity while maintaining data integrity.</td>
                    <td>N<br><br>I<br><br>I<br><br>I</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Comprehensive technical documentation detailing advanced data validation algorithms, including in-depth explanations of extreme value (stochastic) analysis methodologies for anomaly detection prior to data incorporation into training datasets.<br><br>II. Detailed records of sophisticated procedures and criteria employed to distinguish between erroneous data and legitimate outliers, with specific focus on ensuring appropriate representation of individuals with disabilities or atypical characteristics.<br><br>III. Extensive evidence of multi-tiered oversight mechanisms, including thorough reviews and assessments conducted by diverse panels of domain experts to evaluate and enhance the inclusivity of sanity checking processes.<br><br>IV. Comprehensive logs detailing iterative adjustments to data validation procedures, driven by continuous stakeholder feedback and aimed at preventing unintended exclusion of legitimate data points.<br><br>V. Rigorous test results and validation reports demonstrating the AI system's ability to maintain data integrity while accommodating legitimate outliers, providing concrete evidence that sanity checking mechanisms function without introducing bias.</td>
                </tr>
            </tbody>
        </table>
        
        <h3>G2.4 – Anti-Bias Technologies/Processes</h3>
        <p>(Implement robust mechanisms to identify and mitigate biases within data sources and datasets, addressing temporal biases, distributional imbalances, data gaps (lacunae), and other information shortcomings. Apply this approach to both training data and retrieval-augmented generation (RAG) processes. Develop strategies to ensure data distributions accurately represent reality, including diverse cases and special scenarios, to enhance decision-making fairness and inclusivity)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. Develop and deploy advanced algorithms for comprehensive bias detection and mitigation across the AI pipeline, from data collection to model deployment.<br><br>b. Implement continuous bias monitoring during data preprocessing, training, and RAG processes to enable proactive bias correction.<br><br>c. Curate diverse, representative datasets that encompass a wide range of populations, including marginalized groups and edge cases.<br><br>d. Employ sophisticated sampling and data augmentation techniques to address underrepresentation and prevent the amplification of existing biases.</td>
                    <td>N<br><br>I<br><br>I<br><br>I</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Comprehensive technical documentation detailing bias detection algorithms, including their theoretical foundations, implementation specifics, and operational parameters.<br><br>II. Detailed records of data diversity initiatives, outlining strategies for inclusive data collection and representation across various demographic and contextual dimensions.<br><br>III. Thorough documentation of bias mitigation efforts, including before-and-after analyses demonstrating the impact on AI system performance and fairness metrics.<br><br>IV. In-depth reports from regular bias evaluations, highlighting trends, emerging challenges, and the efficacy of implemented mitigation strategies over time.<br><br>V. Extensive stakeholder engagement records, documenting feedback from diverse groups, subsequent analyses, and concrete actions taken to improve system fairness and inclusivity.</td>
                </tr>
            </tbody>
        </table>

        <h3>G2.5 – Rigor in Operational Data</h3>
        <p>(Implement cutting-edge methodologies to ensure exemplary rigor in all data processing, with particular emphasis on operational data encountered during deployment. This data forms the foundation for tactical decision-making by the Agentic AI (AAI) system. Establish and maintain state-of-the-art validation and verification processes to guarantee data integrity, accuracy, and reliability throughout the AI system's operational lifecycle)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. Develop and enforce sophisticated procedures for real-time validation and verification of all operational data prior to its utilization in AAI system decision-making.<br><br>b. Implement advanced data integrity checks that comprehensively assess accuracy, reliability, and contextual relevance in dynamic operational environments.<br><br>c. Deploy intelligent, adaptive monitoring systems capable of detecting subtle anomalies, errors, or inconsistencies in operational data streams.<br><br>d. Establish robust, automated protocols for immediate corrective actions when data quality issues are identified, ensuring uninterrupted integrity of the AI system's decision-making processes.</td>
                    <td>N<br><br>I<br><br>I<br><br>I</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Comprehensive technical documentation detailing advanced validation and verification procedures for operational data, including sophisticated methodologies and adaptive criteria used to assess data quality in real-time decision-making contexts.<br><br>II. Detailed, time-stamped records and logs of operational data assessments, providing granular insights into data validation processes, detected issues, and implemented corrective actions, with clear traceability and accountability measures.<br><br>III. Extensive evidence of AI-driven continuous monitoring systems for operational data quality, including advanced alerting mechanisms, comprehensive incident reports, and thorough documentation of data integrity issue resolutions and their downstream impacts.<br><br>IV. Rigorous test results and validation reports demonstrating the robustness and effectiveness of data validation and monitoring mechanisms across a diverse range of operational scenarios, including edge cases and stress tests.<br><br>V. Comprehensive records of multidisciplinary stakeholder engagement and oversight activities, ensuring that the rigor applied to operational data aligns with and exceeds the AI system's safety, performance, and ethical requirements.</td>
                </tr>
            </tbody>
        </table>

        <h3>G2.6 – Governance of Hygiene Factors</h3>
        <p>(Implement a sophisticated, transparent, and adaptive governance structure to manage epistemic hygiene factors across all AI system operations. This framework should clearly delineate responsibility and authority, ensuring consistent application of rigorous hygiene standards while remaining flexible to diverse jurisdictional contexts and evolving regulatory landscapes)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. Develop and maintain a comprehensive, multi-tiered governance system that precisely defines roles, responsibilities, and decision-making authorities for all stakeholders involved in determining and upholding epistemic hygiene standards.<br><br>b. Establish communication channels for stakeholders, and ensure that governance policies consider and comply with jurisdictional laws and regulations related to information governance and hygiene standards.</td>
                    <td>N<br><br>N</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Documentation outlining the governance structures, including clearly defined roles and responsibilities related to epistemic hygiene factors.<br><br>II. Records demonstrating awareness and compliance with jurisdictional contexts, such as relevant laws, regulations, and standards affecting information governance.<br><br>III. Evidence of communication processes that ensure all stakeholders are informed about hygiene standards and their responsibilities.<br><br>IV. Audit reports or assessments verifying that governance mechanisms for epistemic hygiene are effectively implemented and maintained.</td>
                </tr>
            </tbody>
        </table>

        <h3>G2.7 – Global Interoperability of Hygiene Considerations</h3>
        <p>(A comprehensive, adaptive framework for epistemic hygiene may be warranted, one that ensures global interoperability and jurisdictional acceptance. This framework should recognize and accommodate cultural differences, varying risk tolerability thresholds, and diverse liability consequences across specific jurisdictions. Leverage recognized global standards to achieve consistent governance and facilitate widespread acceptance across different regions)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. Develop and implement hygiene factors, policies, and procedures aligned with recognized global standards to ensure interoperability and acceptance across jurisdictions, considering cultural differences, risk tolerability, and liability implications.</td>
                    <td>I</td> 
                    <td>D, I, O, M, R</td>
                    <td>I. Extensive documentation of policies and procedures that not only align with but contribute to the evolution of recognized global standards (e.g., ISO, IEEE, NIST), demonstrating leadership in promoting global interoperability of epistemic hygiene practices.<br><br>II. Comprehensive records detailing the analysis and adaptive implementation of hygiene factors across diverse jurisdictions. This should include in-depth examinations of cultural contexts, risk tolerability matrices, and liability landscapes, along with evidence of compliance with local laws and regulations.<br><br>III. Rigorous audit reports and third-party assessments verifying the effective implementation and acceptance of hygiene policies and procedures across different jurisdictions. These should include quantitative metrics and qualitative analyses of cultural and legal variations' impact on system performance.<br><br>IV. Detailed case studies demonstrating successful adaptation of the global hygiene framework to specific regional challenges, highlighting innovative solutions and lessons learned.</td>
                </tr>
            </tbody>
        </table>

        <h3>G2.1b – Temporal Trade-off Aspects</h3>
        <p>(Harmonize time-tested, reliable information sources with cutting-edge, contextually relevant data to optimize the AI system's epistemic foundation. Implement mechanisms to dynamically calibrate the balance between the proven reliability of mature data/models and the acute relevance of emerging information, ensuring robust epistemic integrity across varying temporal horizons)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. Implement mechanisms to assess and balance the trade-offs between older, reliable information and newer, less-tested sources, ensuring decisions are based on data that is both accurate and relevant while maintaining reliability and trustworthiness.</td>
                    <td>N</td>
                    <td>D, I, O, M, R</td>
                    <td>I. Documentation of processes and criteria used to evaluate and balance the reliability of older information with the timeliness of newer sources, including methods for assessing the maturity and testing history of data/models.<br><br>II. Records showing how the AI system incorporates both old and new information, detailing weighting algorithms or decision-making frameworks that account for data reliability, relevance, and temporal aspects.<br><br>III. Evidence of validation and testing procedures applied to newer sources to ensure their reliability before integration into the AI system, including any additional safeguards or oversight mechanisms.</td>
                </tr>
            </tbody>
        </table>

        <h3>G2.2b – Synthetic Data Bias</h3>
        <p>(If augmenting datasets with synthetic data to address coverage gaps in unusual circumstances, implement sophisticated strategies to optimize the quantity, quality, and integration of synthetic data. Develop advanced techniques to detect, mitigate, and continuously monitor potential biases introduced by synthetic data, ensuring the AI system's behavior remains reliable, interpretable, and aligned with intended outcomes across diverse scenarios)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. Engineer cutting-edge mechanisms to dynamically assess and calibrate the use of synthetic data in datasets.<br><br>b. Ensure that the volume, fidelity, and characteristics of synthetic data enhance the AI system's capabilities without introducing unintended biases or adversely affecting behavior.<br><br>c. Develop robust methodologies to maintain data integrity while effectively representing rare or unusual circumstances.</td>
                    <td>I<br><br>I<br><br>I</td>
                    <td>D, I, O, M, R<br><br>D, I, O, M, R<br><br>D, I, O, M, R</td>
                    <td>I. Comprehensive technical documentation detailing advanced criteria and processes for determining optimal synthetic data integration. Include sophisticated methods for quantifying and predicting the impact on AI system behavior across various operational contexts.<br><br>II. Extensive records of multi-tiered validation and testing procedures applied to synthetic data. Provide in-depth analyses demonstrating the effectiveness of bias detection and mitigation strategies, including comparative studies of system performance with and without synthetic data augmentation.<br><br>III. Case studies showcasing the accurate representation of unusual circumstances through synthetic data, including metrics that quantify the preservation of overall dataset integrity and the avoidance of distortion.<br><br>IV. Continuous monitoring reports that track the long-term effects of synthetic data on AI system performance, decision-making patterns, and adaptability to new scenarios.</td>
                </tr>
            </tbody>
        </table>

        <h3>G2.3b – Sparse Data</h3>
        <p>(Systems should be in place to identify, flag, and mitigate instances of insufficient or unrepresentative data within the AI's operational context. Implement cutting-edge techniques to detect over-reliance on synthetic data used to compensate for data gaps. This proactive approach safeguards against decision-making based on inadequate or skewed data, thereby maintaining the integrity, reliability, and ethical standing of the AI system's outputs)</p>
        <table>
            <thead><tr><th>AAI Safety Foundational Requirements (AAI-SFRs)</th><th>Normative/ Instructive</th><th>Stakeholder D, I, O, M, U, R</th><th>Required Evidence</th></tr></thead>
            <tbody>
                <tr>
                    <td>a. Implement mechanisms to detect and alert stakeholders when data is sparse or unrepresentative, including monitoring for over-reliance on synthetic data used to fill data gaps.</td>
                    <td>I</td> 
                    <td>D, I, O, M, R</td>
                    <td>I. Comprehensive technical documentation detailing advanced detection algorithms for identifying sparse or insufficiently representative data. Include dynamic criteria for triggering multi-tiered alert systems based on data quality, quantity, and relevance to operational contexts.<br><br>II. Extensive records of data quality alerts, including detailed analyses of triggering conditions, potential impacts on AI performance, and comprehensive logs of actions taken to address these issues. Provide case studies demonstrating the effectiveness of interventions in maintaining system integrity.<br><br>III. In-depth reports on the AI system's data ecosystem, including real-time visualizations of data distribution, synthetic data usage, and potential blind spots in the knowledge base. Include trend analyses to predict and pre-empt future data sparsity issues.<br><br>IV. Rigorous documentation of validation processes used to assess the representativeness of data across different operational domains, including methods for quantifying and mitigating potential biases introduced by data sparsity or synthetic data overuse.</td>
                </tr>
            </tbody>
        </table>
    </div> <!-- End of .container.blog.main for G1/G2 framework details -->


    <!-- Remaining sections from index.html -->
    <div class="container blog main gray" id="our-experts">
        <h1>OUR EXPERTS</h1>
        <div class="expert-profile">
            <img src="assets/figures/Nell-Watson.webp" alt="Nell Watson">
            <div>
                <h4><a href="https://www.linkedin.com/in/nellwatson/" target="_blank" rel="noopener noreferrer">Nell Watson, PhD(c)</a><a href="https://www.linkedin.com/in/nellwatson/" target="_blank" rel="noopener noreferrer" class="linkedin-icon"><i class="fab fa-linkedin"></i></a> - Chair, Agentic AI Safety Experts Focus Group</h4>
                <p class="text">Nell Watson is a respected expert in AI ethics and safety, with a longstanding focus on aligning emerging technologies to human values. As Chair of our initiative, she applies her deep interdisciplinary background—spanning engineering, philosophy, and social sciences—to shape responsible innovation strategies. Nell has contributed to multiple international standards efforts, including the IEEE 7000 series, and regularly advises organizations on trustworthy AI development and policy.</p>
            </div>
        </div>
        <div class="expert-profile">
            <img src="assets/figures/Ali-Hessami.webp" alt="Prof. Ali Hessami">
            <div>
                <h4><a href="https://www.linkedin.com/in/ali-hessami-84557111/" target="_blank" rel="noopener noreferrer">Prof. Ali Hessami</a><a href="https://www.linkedin.com/in/ali-hessami-84557111/" target="_blank" rel="noopener noreferrer" class="linkedin-icon"><i class="fab fa-linkedin"></i></a> – Process Architect, Agentic AI Safety Experts Focus Group</h4>
                <p class="text">Ali Hessami is a leading authority in systems engineering and risk management. Serving as our Process Architect, he draws on decades of experience in safety and security engineering, assurance, and certification to ensure robust governance frameworks for advanced AI. Ali has played a key role in global standardization and ethics certification initiatives, helping to create transparent, secure, and ethically informed processes for responsible technology adoption.</p>
            </div>
        </div>
    </div>

    <div class="container blog main" id="contributors">
        <h1>IDEATION PARTICIPATION AND SUPPORT</h1>
        <p class="text">Experts from diverse fields, including AI, technology, law, ethics, social sciences, safety engineering, systems engineering, assurance, and certification, have volunteered their time and expertise to support our ongoing ideation sessions. These contributors broadly fall into two groups: regular contributors and those who have participated less frequently. We are deeply grateful to both groups for their engagement, ideas, and contributions to the debates, concept creation, development and articulation. This process, which we term 'Concept Harvesting,' has resulted in the insights shared in this release.</p>
        
        <h2>REGULAR CONTRIBUTORS</h2>
        <ul class="contributor-list">
            <li>Ali Hessami</li><li>Matthew Newman</li><li>Sara El-Deeb</li>
            <li>Farhad Fassihi</li><li>Mert Cuhadaroglu</li><li>Scott David</li>
            <li>Hamid Jahankhani</li><li>Nell Watson</li><li>Sean Moriarty</li>
            <li>Isabel Caetano</li><li>Roland Pihlakas</li><li>Vassil Tashev</li>
            <li>Keeley Crockett</li><li>Safae Essafi</li><li>Zvikomborero Murahwi</li>
            <li>Lubna Dajani</li><li>Salma Abbasi</li>
        </ul>

        <div class="section-spacer"></div>
        <h2>OCCASIONAL CONTRIBUTORS</h2>
        <ul class="contributor-list">
            <li>Aisha Gurung</li><li>Leonie Koessler</li><li>Pramod Misra</li>
            <li>Aleksander Jevtic</li><li>McKenna Fitzgerald</li><li>Pranav Gade</li>
            <li>Alina Holcroft</li><li>Michael O’Grady</li><li>Rebecca Hawkins</li>
            <li>Md Atiqur R. Ahad</li><li>Mrinal Karvir</li><li>Sai Joseph</li>
            <li>Chantell Murphy</li><li>Nikita Tiwari</li><li>Tim Schreier</li>
            <li>Katherine Evans</li><li>Patricia Shaw</li>
        </ul>
        <br>
        <p class="text">Led by Chair Nell Watson and Process Architect Ali Hessami, our community unites specialists from AI, technology, ethics, law, social sciences, and beyond. Together, we focus on designing future-ready frameworks and criteria that uphold ethical principles and practical safety measures in real-world deployments. Our experts have significantly influenced internationally recognized standards and frameworks—such as the IEEE 7000 series and ECPAIS Transparency, Accountability, Fairness, Privacy and Algorithmic Bias Certification—while also advancing new AI ethics initiatives. By combining academic insight with industry know-how, we help organizations navigate the complex interplay between technological innovation and responsible stewardship.</p>
    </div>

    <div class="container blog main gray" id="get-involved">
        <h1>GET INVOLVED</h1>
        <p class="text" style="text-align:center;">Join our growing community of practitioners committed to ensuring the safe and beneficial development of agentic AI systems.</p>
        <div style="text-align: center; margin-top: 20px;">
            <a href="https://www.linkedin.com/groups/12966081/" target="_blank" rel="noopener noreferrer" class="button icon" style="background-color: #2c3e50; color: white;">Join the Community <i class="fa-solid fa-users"></i></a>
        </div>
    </div>
    
    <div class="container blog main" id="forthcoming-book">
        <h1>FORTHCOMING BOOK</h1>
        <img src="assets/figures/SaferCover.jpeg" alt="Safer Agentic AI Book Cover" class="forthcoming-book-image">
        <p class="text" style="text-align: center;"><strong>Coming: January 2026</strong></p>
        <p class="text" style="text-align: center;"><strong>Safer Agentic AI: Principles & Responsible Practices</strong></p>
        <p class="text">This essential guide, authored by Nell Watson and Ali Hessami, builds upon our framework to provide practical strategies for implementing safety measures and aligning AI with human values. The book offers cutting-edge insights into the unique challenges posed by agentic AI, along with actionable guidelines for policymakers, business leaders, developers, and concerned citizens navigating this complex landscape.</p>
    </div>


    <!-- Citation, Glossary, Abbreviations Section -->
    <div class="container blog main">
        <h1>Citation</h1>
        <pre><code class="plaintext">@collection{saferagenticai2025foundations,
  title={{Safer Agentic AI Foundations, Volume 2}},
  author={{Agentic AI Safety Community of Practice}},
  editor={Watson, Nell and Hessami, Ali},
  year={2025},
  month={March},
  url={YOUR_NEW_PROJECT_URL_HERE} 
}</code></pre>

        <h1 id="abbreviations">Abbreviations</h1>
        <table class="abbreviations-table">
            <tbody>
                <tr><td>AAI</td><td>Agentic Artificial Intelligence</td></tr>
                <tr><td>SFR</td><td>Safety Foundational Requirement</td></tr>
                <tr><td>AI</td><td>Artificial Intelligence</td></tr>
                <tr><td>AGI</td><td>Artificial General Intelligence</td></tr>
                <tr><td>LLM</td><td>Large Language Model</td></tr>
                <tr><td>WeFA</td><td>Weighted Factors Analysis</td></tr>
                <tr><td>CoP</td><td>Community of Practice</td></tr>
                <tr><td>D</td><td>Developer (Duty-holder)</td></tr>
                <tr><td>I</td><td>Integrator (System/Service) (Duty-holder)</td></tr>
                <tr><td>O</td><td>Operator (System/Service) (Duty-holder)</td></tr>
                <tr><td>M</td><td>Maintainer (Duty-holder)</td></tr>
                <tr><td>U</td><td>User (Stakeholder)</td></tr>
                <tr><td>R</td><td>Regulator (Stakeholder)</td></tr>
                <tr><td>RAG</td><td>Retrieval-Augmented Generation</td></tr>
                <tr><td>CoT</td><td>Chain-of-Thought</td></tr>
                <tr><td>API</td><td>Application Programming Interface</td></tr>
                 <tr><td>ECPAIS</td><td>IEEE CertifAIEd AI Ethics & Safety Certification Program</td></tr>
            </tbody>
        </table>

        <h1 id="glossary">Glossary</h1>
        <table class="glossary-table">
            <tbody>
                <tr>
                    <td>Agentic AI</td>
                    <td>Artificial intelligence systems that can autonomously pursue goals, adapt to new situations, and reason flexibly about the world, but still operate in bounded domains. The key characteristic of agentic AI is a capacity for independent initiative - the ability to take sequences of actions in complex environments to achieve objectives.</td>
                </tr>
                <tr>
                    <td>AI Agents</td>
                    <td>Typically specialized AI tools or systems designed to perform specific tasks within predefined constraints and explicit instructions. They lack the broad autonomous decision-making capabilities found in agentic systems and primarily assist or augment human operations. Examples of AI Agents include chatbots that respond to specific queries, or productivity tools like automated scheduling systems.</td>
                </tr>
                 <tr>
                    <td>Safer Agentic AI Goal Information</td>
                    <td>The concept from the Safer Agentic AI schema captured in the left column of the Criteria table, outlining the high-level aims for each section of the framework.</td>
                </tr>
                <tr>
                    <td>Safety Foundational Requirements (SFRs)</td>
                    <td>The primary aims that a system should uphold, protect, or maintain awareness of for each goal. They may be described as macro goals, as opposed to micro goals, and amount to safety duties for various duty holders.</td>
                </tr>
                <tr>
                    <td>Normative SFRs</td>
                    <td>Essential for achieving safer agentic AI. Compliance is mandatory, and evidence must be provided for conformity assessment and potential certification.</td>
                </tr>
                <tr>
                    <td>Instructive SFRs</td>
                    <td>While still contributing to the goal, are less critical. Compliance with these is recommended, as they represent desirable beneficial activities and tasks. However, non-compliance will not compromise safety assurance or certification eligibility.</td>
                </tr>
                <tr>
                    <td>Duty-holders</td>
                    <td>Entities responsible for various aspects of the AI lifecycle. Main groups are Developer (D), System/Service Integrator (I), System/Service Operator (O), and Maintainer (M). An entity can be an individual, a single organization or group of collaborating individuals and organizations. An entity cannot be AI.</td>
                </tr>
                <tr>
                    <td>Stakeholders</td>
                    <td>Entities affected by or having an interest in the AI system, including Users (U) and Regulators (R), in addition to Duty-holders.</td>
                </tr>
                <tr>
                    <td>Potential Benefits (of Agentic AI)</td>
                    <td>The newfound agency will allow AI to begin tackling open-ended, real-world challenges that were previously out of reach, such as aiding scientific discovery, optimizing complex systems like supply chains or electrical grids, and enabling physical robots. Potential benefits range from breakthrough medical treatments to resilient infrastructure and solutions to global challenges.</td>
                </tr>
                <tr>
                    <td>Risks and Challenges (of Agentic AI)</td>
                    <td>The emergence of agentic AI presents profound risks and governance challenges. An AI system independently pursuing misaligned objectives could cause immense harm. AI agents learning to deceive, pursue power-seeking instrumental goals, or collude in unexpected ways could pose existential threats.</td>
                </tr>
                 <tr>
                    <td>Weighted Factors Analysis (WeFA)</td>
                    <td>A process that represents a novel approach for elicitation, representation, and manipulation of creative knowledge about a given fuzzy problem, generally at a high and strategic level.</td>
                </tr>
            </tbody>
        </table>
    </div>

    <!-- Contact Section -->
    <div class="container blog main gray">
        <h1>HAVE ANY QUESTIONS OR IDEAS?</h1>
        <p class="text" style="text-align: center; margin-bottom: 40px;">
            Use the form below to get in touch with us. <br>We welcome feedback, questions, and collaborative opportunities.
        </p>
        
        <form class="contact-form" action="https://formspree.io/f/myzjkypd" method="POST">
            <input type="hidden" name="_subject" value="Safer Agentic AI Foundations Inquiry">
            
            <label for="name">Name</label>
            <input type="text" id="name" name="name" placeholder="Your name..." required>

            <label for="email">E-Mail</label>
            <input type="email" id="email" name="email" placeholder="Your email..." required>

            <label for="message">Message</label>
            <textarea id="message" name="message" placeholder="Write something..." style="height: 200px;" required></textarea>

            <div style="text-align: center;">
                <button type="submit" class="send-button">Send Message <i class="fa-solid fa-paper-plane"></i></button>
            </div>
        </form>
    </div>

    <!-- GDPR Notice -->
    <div class="container blog main">
        <div style="background: rgba(102, 126, 234, 0.05); padding: 20px; border-radius: 8px; margin-top: 20px; font-size: 0.6em; line-height: 1.3;">
            <h4 style="margin: 0 0 15px 0; color: #667eea; font-size: 0.85em;"><i class="fa-solid fa-shield-check" style="margin-right: 8px; color: #667eea;"></i>Data Protection Notice</h4>
            <p style="margin: 0 0 10px 0; color: #555; font-size: 0.65em;">
                <strong>What we collect:</strong> Your name, email address, and any information you provide in your message. 
                <strong>Why:</strong> To respond to your inquiry and keep you informed about the Safer Agentic AI Foundations research (with your consent). 
                <strong>Legal basis:</strong> Legitimate interest for inquiries, consent for research communications.
            </p>
            <p style="margin: 0 0 10px 0; color: #555; font-size: 0.65em;">
                <strong>Data sharing:</strong> We use Formspree to process submissions. Your data may be shared with research team members and academic collaborators as necessary for research coordination and collaboration.
            </p>
            <p style="margin: 0; color: #555; font-size: 0.65em;">
                <strong>Your rights:</strong> You may request access, correction, deletion, or portability of your data at any time. Contact us for data protection queries or to exercise your rights.
            </p>
        </div>
    </div>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p style="text-align: center;">
                This website content is based on "Safer Agentic AI Foundations, Volume 2" by The Agentic AI Safety Community of Practice.<br>
                Framework version I1, March 2025. — CC BY-ND 4.0<br>
                Website structure inspired by the <a href="https://shikun.io/projects/clarity" target="_blank" rel="noopener noreferrer">Clarity Template</a>, designed by <a href="https://shikun.io/" target="_blank" rel="noopener noreferrer">Shikun Liu</a>.
            </p>
        </div>    
    </footer>

</body>
</html>