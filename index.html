<!DOCTYPE html>
<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Safer Agentic AI Foundations: A Framework for Responsible Governance</title>
    <meta name="description" content="A comprehensive framework for the responsible governance and safety of advanced Agentic AI systems, by the Agentic AI Safety Community of Practice.">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_GB" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="YOUR_NEW_PROJECT_URL_HERE" property="og:url"> 
    <meta content="Safer Agentic AI Foundations: A Framework for Responsible Governance" property="og:title">
    <meta content="A comprehensive framework for the responsible governance and safety of advanced Agentic AI systems." property="og:description">
    <meta content="YOUR_NEW_PROJECT_URL_HERE/assets/figures/AI_Safety_Logo-Color.png" property="og:image">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@NellWatson"> 
    <meta name="twitter:title" content="Safer Agentic AI Foundations: A Framework for Responsible Governance">
    <meta name="twitter:description" content="A comprehensive framework for the responsible governance and safety of advanced Agentic AI systems.">
    <meta name="twitter:image" content="YOUR_NEW_PROJECT_URL_HERE/assets/figures/AI_Safety_Logo-Color.png">
    
    <link rel="icon" type="image/x-icon" href="assets/figures/favicon.ico">
    <link rel="icon" type="image/svg+xml" href="assets/figures/favicon.svg">
    <link rel="icon" type="image/png" sizes="96x96" href="assets/figures/favicon-96x96.png">
    <link rel="apple-touch-icon" sizes="180x180" href="assets/figures/apple-touch-icon.png">
    
    <link rel="icon" type="image/png" sizes="192x192" href="assets/figures/clarity_light.png" media="(prefers-color-scheme: light)">
    <link rel="icon" type="image/png" sizes="192x192" href="assets/figures/clarity_dark.png" media="(prefers-color-scheme: dark)">
    
    <link rel="manifest" href="assets/figures/site.webmanifest">
    <meta name="theme-color" content="#2c3e50">
    <meta name="background-color" content="#ffffff">
    
    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
    <link rel="stylesheet" type="text/css" media="all" href="psychopathia-styles.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <!-- navbar.js is usually for the page it's on, so it might be omitted here if ToC is only for framework.html -->
    <!-- For now, keeping it, but it won't find H1s for Drivers/Inhibitors on *this* page -->
    <script src="assets/scripts/navbar.js"></script> 
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": { scale: 95, fonts: ["Gyre-Pagella"], imageFont: null, undefinedFamily: "'Arial Unicode MS', cmbright" },
            tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true }
          });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <!-- No JS needed here for H1/H3 wrapping as framework is on separate page -->
    <style>
        .expert-profile { display: flex; align-items: flex-start; margin-bottom: 30px; }
        .expert-profile img { width: 120px; height: 120px; border-radius: 50%; margin-right: 20px; object-fit: cover; border: 2px solid #dde5eb; }
        .expert-profile div { flex: 1; }
        .expert-profile h4 { margin-top: 0; color: #2c3e50; margin-bottom: 0.5em;}
        .expert-profile h4 a { color: #2c3e50; text-decoration: none; }
        .expert-profile h4 a:hover { text-decoration: underline; }
        .expert-profile h4 a.linkedin-icon {
            margin-left: 8px; 
            font-size: 1em; 
            color: #0077b5; 
            text-decoration: none; 
            border-bottom: none; 
            transition: color 0.2s ease-in-out;
        }
        .expert-profile h4 a.linkedin-icon:hover {
            color: #DAA520; 
        }
        .expert-profile h4 a.linkedin-icon i {
            vertical-align: baseline; 
        }
        .expert-profile p.text { font-size: 0.9em; line-height: 1.6; }
        .contributor-list { columns: 3; -webkit-columns: 3; -moz-columns: 3; list-style-type: none; padding-left: 0;}
        .contributor-list li { margin-bottom: 8px; font-size: 0.9em;}
        @media (max-width: 768px) { .contributor-list { columns: 2; -webkit-columns: 2; -moz-columns: 2;} }
        @media (max-width: 480px) { .contributor-list { columns: 1; -webkit-columns: 1; -moz-columns: 1;} }
        .key-focus-area { margin-bottom: 20px; }
        .key-focus-area h4 { color: #2c3e50; margin-bottom: 5px;}
        .key-focus-area p { font-size: 0.9em; color: #555; margin-top: 0;}
        .forthcoming-book-image { max-width: 250px; display: block; margin: 0 auto 20px auto; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);}
        .section-spacer { margin-top: 2em; }
        .big-framework-button {
            display: inline-block;
            padding: 15px 30px;
            font-size: 1.2em;
            font-weight: bold;
            color: white;
            background: linear-gradient(135deg, #3498db 0%, #2980b9 100%); /* Example blue gradient */
            border: none;
            border-radius: 8px;
            text-decoration: none;
            text-align: center;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
            margin-top: 20px;
        }
        .big-framework-button:hover {
            background: linear-gradient(135deg, #2980b9 0%, #3498db 100%);
            box-shadow: 0 6px 20px rgba(0,0,0,0.3);
            transform: translateY(-2px);
        }
        .big-framework-button i {
            margin-right: 10px;
        }
    </style>
</head>
<body>

    <!-- Title Page -->
    <div class="container blog" id="first-content" style="background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);">
        <div class="blog-title white">
            <div class="blog-intro">
                <div>
                    <h1 class="title">
                        <span class="title-first-line">SAFER AGENTIC AI FOUNDATIONS</span>
                        <span class="title-second-line">Building the essential framework for responsible governance of advanced AI systems</span>
                    </h1>
                    <p class="author">by The Agentic AI Safety Community of Practice<br>(Nell Watson, Chair; Prof. Ali Hessami, Process Architect)</p>
                    <p class="abstract">
                        Welcome to this second, full volume of our Safer Agentic AI Foundations, guidelines and best practices 
                        for AI systems capable of significant independent action at arm’s length human influence. Our Working Group of 25 experts has 
                        employed a Weighted Factors Methodology to ideate, define, analyze and map the factors which can drive or inhibit 
                        safety in agentic systems, based on fundamental principles. While this document primarily addresses agentic AI, many 
                        underlying principles should also have value for non-agentic systems. We hope this exploration provides a strengthened 
                        awareness of the complexities involved in safer agentic AI.
                    </p>
                </div>
               
                <div class="info">
                    <div>
                        <!-- "Framework" button now links to framework.html -->
                        <a href="framework.html#driver-g1-goal-alignment" class="button icon">Framework <i class="fa-solid fa-sitemap"></i></a>
                        <a href="Safer_Agentic_AI_Foundations_Vol2_I1_March2025.pdf" target="_blank" rel="noopener noreferrer" class="button icon">Hard Copy <i class="fa-solid fa-book-open"></i></a>
                        <a href="#key-focus-areas" class="button icon">Focus Areas <i class="fa-solid fa-bullseye"></i></a>
                        <a href="#glossary" class="button icon">Glossary <i class="fa-solid fa-list-check"></i></a>
                    </div>
                </div>
            </div>

            <div class="blog-cover">
                <img class="foreground" src="assets/figures/AI_Safety_Logo-Color.png" alt="Safer Agentic AI Logo">
                <img class="background" src="assets/figures/AI_Safety_Logo-Color.png" alt="Safer Agentic AI Logo">
            </div>
        </div>
    </div>

    <div class="container blog main first" id="blog-main">
        <h1>ABOUT THE AGENTIC AI SAFETY COMMUNITY</h1>
        <p class="text">The Safer Agentic AI Community of Practice brings together leading experts from diverse fields to establish comprehensive safety guidelines for AI systems capable of independent action and decision-making.</p>
        <p class="text">As artificial intelligence evolves toward greater autonomy, our mission becomes increasingly significant: to develop robust, implementable frameworks that ensure agentic AI systems remain aligned with human values and operate safely across all contexts.</p>
        
        <h1>WHAT IS AGENTIC AI?</h1>
        <p class="text">Agentic AI refers to an important intermediate category: AI systems that can autonomously pursue goals, adapt to new situations, and reason flexibly about the world, but still operate in bounded domains. The key characteristic of agentic AI is a capacity for independent initiative—the ability to take sequences of actions in complex environments to achieve objectives. This can include breaking down high-level goals into subtasks, engaging in open-ended exploration and experimentation, and adapting creatively to novel challenges. By scaffolding capabilities like reasoning, planning, and self-checking on top of large language models, researchers are creating powerful agentic AI systems that can independently make and execute multi-step plans flexibly adjusting strategies based on experience and environmental feedback to achieve objectives.</p>
        <p class="text">Examples of Agentic AI include autonomous driving systems that continuously adapt to changing road conditions, or complex supply chain management systems that autonomously optimize resource allocation in dynamic environments.</p>

        <h1>OUR WORK</h1>
        <p class="text">In September 2024, our Working Group of 25 experts released Volume 1 of the “Safer Agentic AI Foundations” Recommended Practices as a preliminary Creative Commons publication.</p>
        <p class="text">After completion of the extensive ideation sessions and research, we are now publishing the Volume 2—a comprehensive framework of recommended practices which address the drivers and inhibitors of safety in agentic systems. Our Working Group has employed a Weighted Factors Methodology to ideate, define, analyze and map the factors which can drive or inhibit safety in agentic systems, based on fundamental principles. We have used this same process many times previously to generate a range of global standards, certifications, and guidelines for improving ethical qualities in AI systems.</p>
        <div style="text-align: center; margin: 30px 0;">
             <a href="framework.html" class="big-framework-button"><i class="fa-solid fa-sitemap"></i> Explore the Full Framework</a>
        </div>

        <h1>VISUALIZING THE FRAMEWORK</h1>
        <img src="assets/figures/diagram3_v3-1677x2048.webp" alt="Diagram of Safer Agentic AI Foundations: Drivers and Inhibitors">
        <p class="caption" style="text-align: center;">
            Figure 1: Weighted Drivers and Inhibitors for Achieving Safer Agentic AI Systems. 
        </p>
    </div>

    <div class="container blog main gray" id="key-focus-areas">
        <h1>KEY FOCUS AREAS</h1>
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px;">
            <div class="key-focus-area">
                <h4>Goal Alignment</h4>
                <p>Ensuring robust alignment between operational goals and human values.</p>
            </div>
            <div class="key-focus-area">
                <h4>Value Alignment</h4>
                <p>Identifying, codifying, and maintaining human values in AI systems.</p>
            </div>
            <div class="key-focus-area">
                <h4>Safe Operations</h4>
                <p>Ensuring safe operations throughout the system lifecycle.</p>
            </div>
            <div class="key-focus-area">
                <h4>Epistemic Hygiene</h4>
                <p>Maintaining cognitive clarity and accurate information management.</p>
            </div>
            <div class="key-focus-area">
                <h4>Transparency</h4>
                <p>Creating clear, interpretable rationales for AI reasoning processes.</p>
            </div>
            <div class="key-focus-area">
                <h4>Goal Termination</h4>
                <p>Implementing proper protocols for task completion and system sunsetting.</p>
            </div>
            <div class="key-focus-area">
                <h4>Security</h4>
                <p>Implementing comprehensive protection against threats and vulnerabilities.</p>
            </div>
            <div class="key-focus-area">
                <h4>Contextual Understanding</h4>
                <p>Establishing robust control mechanisms across operational contexts.</p>
            </div>
        </div>
    </div>
    
    <div class="container blog main gray" id="our-experts">
        <h1>OUR EXPERTS</h1>
        <div class="expert-profile">
            <img src="assets/figures/Nell-Watson.webp" alt="Nell Watson">
            <div>
                <h4><a href="https://www.linkedin.com/in/nellwatson/" target="_blank" rel="noopener noreferrer">Nell Watson, PhD(c)</a><a href="https://www.linkedin.com/in/nellwatson/" target="_blank" rel="noopener noreferrer" class="linkedin-icon"><i class="fab fa-linkedin"></i></a> - Chair, Agentic AI Safety Experts Focus Group</h4>
                <p class="text">Nell Watson is a respected expert in AI ethics and safety, with a longstanding focus on aligning emerging technologies to human values. As Chair of our initiative, she applies her deep interdisciplinary background—spanning engineering, philosophy, and social sciences—to shape responsible innovation strategies. Nell has contributed to multiple international standards efforts, including the IEEE 7000 series, and regularly advises organizations on trustworthy AI development and policy.</p>
            </div>
        </div>
        <div class="expert-profile">
            <img src="assets/figures/Ali-Hessami.webp" alt="Prof. Ali Hessami">
            <div>
                <h4><a href="https://www.linkedin.com/in/ali-hessami-84557111/" target="_blank" rel="noopener noreferrer">Prof. Ali Hessami</a><a href="https://www.linkedin.com/in/ali-hessami-84557111/" target="_blank" rel="noopener noreferrer" class="linkedin-icon"><i class="fab fa-linkedin"></i></a> – Process Architect, Agentic AI Safety Experts Focus Group</h4>
                <p class="text">Ali Hessami is a leading authority in systems engineering and risk management. Serving as our Process Architect, he draws on decades of experience in safety and security engineering, assurance, and certification to ensure robust governance frameworks for advanced AI. Ali has played a key role in global standardization and ethics certification initiatives, helping to create transparent, secure, and ethically informed processes for responsible technology adoption.</p>
            </div>
        </div>
    </div>

    <div class="container blog main" id="contributors">
        <h1>IDEATION PARTICIPATION AND SUPPORT</h1>
        <p class="text">Experts from diverse fields, including AI, technology, law, ethics, social sciences, safety engineering, systems engineering, assurance, and certification, have volunteered their time and expertise to support our ongoing ideation sessions. These contributors broadly fall into two groups: regular contributors and those who have participated less frequently. We are deeply grateful to both groups for their engagement, ideas, and contributions to the debates, concept creation, development and articulation. This process, which we term 'Concept Harvesting,' has resulted in the insights shared in this release.</p>
        
        <h2>REGULAR CONTRIBUTORS</h2>
        <ul class="contributor-list">
            <li>Ali Hessami</li><li>Matthew Newman</li><li>Sara El-Deeb</li>
            <li>Farhad Fassihi</li><li>Mert Cuhadaroglu</li><li>Scott David</li>
            <li>Hamid Jahankhani</li><li>Nell Watson</li><li>Sean Moriarty</li>
            <li>Isabel Caetano</li><li>Roland Pihlakas</li><li>Vassil Tashev</li>
            <li>Keeley Crockett</li><li>Safae Essafi</li><li>Zvikomborero Murahwi</li>
            <li>Lubna Dajani</li><li>Salma Abbasi</li>
        </ul>

        <div class="section-spacer"></div>
        <h2>OCCASIONAL CONTRIBUTORS</h2>
        <ul class="contributor-list">
            <li>Aisha Gurung</li><li>Leonie Koessler</li><li>Pramod Misra</li>
            <li>Aleksander Jevtic</li><li>McKenna Fitzgerald</li><li>Pranav Gade</li>
            <li>Alina Holcroft</li><li>Michael O’Grady</li><li>Rebecca Hawkins</li>
            <li>Md Atiqur R. Ahad</li><li>Mrinal Karvir</li><li>Sai Joseph</li>
            <li>Chantell Murphy</li><li>Nikita Tiwari</li><li>Tim Schreier</li>
            <li>Katherine Evans</li><li>Patricia Shaw</li>
        </ul>
        <br>
        <p class="text">Led by Chair Nell Watson and Process Architect Ali Hessami, our community unites specialists from AI, technology, ethics, law, social sciences, and beyond. Together, we focus on designing future-ready frameworks and criteria that uphold ethical principles and practical safety measures in real-world deployments. Our experts have significantly influenced internationally recognized standards and frameworks—such as the IEEE 7000 series and ECPAIS Transparency, Accountability, Fairness, Privacy and Algorithmic Bias Certification—while also advancing new AI ethics initiatives. By combining academic insight with industry know-how, we help organizations navigate the complex interplay between technological innovation and responsible stewardship.</p>
    </div>

    <div class="container blog main gray" id="get-involved">
        <h1>GET INVOLVED</h1>
        <p class="text" style="text-align:center;">Join our growing community of practitioners committed to ensuring the safe and beneficial development of agentic AI systems.</p>
        <div style="text-align: center; margin-top: 20px;">
            <a href="https://www.linkedin.com/groups/12966081/" target="_blank" rel="noopener noreferrer" class="button icon" style="background-color: #2c3e50; color: white;">Join the Community <i class="fa-solid fa-users"></i></a>
        </div>
    </div>
    
    <div class="container blog main" id="forthcoming-book">
        <h1>FORTHCOMING BOOK</h1>
        <img src="assets/figures/SaferCover.jpeg" alt="Safer Agentic AI Book Cover" class="forthcoming-book-image">
        <p class="text" style="text-align: center;"><strong>Coming: January 2026</strong></p>
        <p class="text" style="text-align: center;"><strong>Safer Agentic AI: Principles & Responsible Practices</strong></p>
        <p class="text">This essential guide, authored by Nell Watson and Ali Hessami, builds upon our framework to provide practical strategies for implementing safety measures and aligning AI with human values. The book offers cutting-edge insights into the unique challenges posed by agentic AI, along with actionable guidelines for policymakers, business leaders, developers, and concerned citizens navigating this complex landscape.</p>
    </div>


    <!-- Citation, Glossary, Abbreviations Section -->
    <div class="container blog main">
        <h1>Citation</h1>
        <pre><code class="plaintext">@collection{saferagenticai2025foundations,
  title={{Safer Agentic AI Foundations, Volume 2}},
  author={{Agentic AI Safety Community of Practice}},
  editor={Watson, Nell and Hessami, Ali},
  year={2025},
  month={March},
  url={YOUR_NEW_PROJECT_URL_HERE} 
}</code></pre>

        <h1 id="abbreviations">Abbreviations</h1>
        <table class="abbreviations-table">
            <tbody>
                <tr><td>AAI</td><td>Agentic Artificial Intelligence</td></tr>
                <tr><td>SFR</td><td>Safety Foundational Requirement</td></tr>
                <tr><td>AI</td><td>Artificial Intelligence</td></tr>
                <tr><td>AGI</td><td>Artificial General Intelligence</td></tr>
                <tr><td>LLM</td><td>Large Language Model</td></tr>
                <tr><td>WeFA</td><td>Weighted Factors Analysis</td></tr>
                <tr><td>CoP</td><td>Community of Practice</td></tr>
                <tr><td>D</td><td>Developer (Duty-holder)</td></tr>
                <tr><td>I</td><td>Integrator (System/Service) (Duty-holder)</td></tr>
                <tr><td>O</td><td>Operator (System/Service) (Duty-holder)</td></tr>
                <tr><td>M</td><td>Maintainer (Duty-holder)</td></tr>
                <tr><td>U</td><td>User (Stakeholder)</td></tr>
                <tr><td>R</td><td>Regulator (Stakeholder)</td></tr>
                <tr><td>RAG</td><td>Retrieval-Augmented Generation</td></tr>
                <tr><td>CoT</td><td>Chain-of-Thought</td></tr>
                <tr><td>API</td><td>Application Programming Interface</td></tr>
                 <tr><td>ECPAIS</td><td>IEEE CertifAIEd AI Ethics & Safety Certification Program</td></tr>
            </tbody>
        </table>

        <h1 id="glossary">Glossary</h1>
        <table class="glossary-table">
            <tbody>
                <tr>
                    <td>Agentic AI</td>
                    <td>Artificial intelligence systems that can autonomously pursue goals, adapt to new situations, and reason flexibly about the world, but still operate in bounded domains. The key characteristic of agentic AI is a capacity for independent initiative - the ability to take sequences of actions in complex environments to achieve objectives.</td>
                </tr>
                <tr>
                    <td>AI Agents</td>
                    <td>Typically specialized AI tools or systems designed to perform specific tasks within predefined constraints and explicit instructions. They lack the broad autonomous decision-making capabilities found in agentic systems and primarily assist or augment human operations. Examples of AI Agents include chatbots that respond to specific queries, or productivity tools like automated scheduling systems.</td>
                </tr>
                 <tr>
                    <td>Safer Agentic AI Goal Information</td>
                    <td>The concept from the Safer Agentic AI schema captured in the left column of the Criteria table, outlining the high-level aims for each section of the framework.</td>
                </tr>
                <tr>
                    <td>Safety Foundational Requirements (SFRs)</td>
                    <td>The primary aims that a system should uphold, protect, or maintain awareness of for each goal. They may be described as macro goals, as opposed to micro goals, and amount to safety duties for various duty holders.</td>
                </tr>
                <tr>
                    <td>Normative SFRs</td>
                    <td>Essential for achieving safer agentic AI. Compliance is mandatory, and evidence must be provided for conformity assessment and potential certification.</td>
                </tr>
                <tr>
                    <td>Instructive SFRs</td>
                    <td>While still contributing to the goal, are less critical. Compliance with these is recommended, as they represent desirable beneficial activities and tasks. However, non-compliance will not compromise safety assurance or certification eligibility.</td>
                </tr>
                <tr>
                    <td>Duty-holders</td>
                    <td>Entities responsible for various aspects of the AI lifecycle. Main groups are Developer (D), System/Service Integrator (I), System/Service Operator (O), and Maintainer (M). An entity can be an individual, a single organization or group of collaborating individuals and organizations. An entity cannot be AI.</td>
                </tr>
                <tr>
                    <td>Stakeholders</td>
                    <td>Entities affected by or having an interest in the AI system, including Users (U) and Regulators (R), in addition to Duty-holders.</td>
                </tr>
                <tr>
                    <td>Potential Benefits (of Agentic AI)</td>
                    <td>The newfound agency will allow AI to begin tackling open-ended, real-world challenges that were previously out of reach, such as aiding scientific discovery, optimizing complex systems like supply chains or electrical grids, and enabling physical robots. Potential benefits range from breakthrough medical treatments to resilient infrastructure and solutions to global challenges.</td>
                </tr>
                <tr>
                    <td>Risks and Challenges (of Agentic AI)</td>
                    <td>The emergence of agentic AI presents profound risks and governance challenges. An AI system independently pursuing misaligned objectives could cause immense harm. AI agents learning to deceive, pursue power-seeking instrumental goals, or collude in unexpected ways could pose existential threats.</td>
                </tr>
                 <tr>
                    <td>Weighted Factors Analysis (WeFA)</td>
                    <td>A process that represents a novel approach for elicitation, representation, and manipulation of creative knowledge about a given fuzzy problem, generally at a high and strategic level.</td>
                </tr>
            </tbody>
        </table>
    </div>

    <!-- Contact Section -->
    <div class="container blog main gray">
        <h1>HAVE ANY QUESTIONS OR IDEAS?</h1>
        <p class="text" style="text-align: center; margin-bottom: 40px;">
            Use the form below to get in touch with us. <br>We welcome feedback, questions, and collaborative opportunities.
        </p>
        
        <form class="contact-form" action="https://formspree.io/f/myzjkypd" method="POST">
            <input type="hidden" name="_subject" value="Safer Agentic AI Foundations Inquiry">
            
            <label for="name">Name</label>
            <input type="text" id="name" name="name" placeholder="Your name..." required>

            <label for="email">E-Mail</label>
            <input type="email" id="email" name="email" placeholder="Your email..." required>

            <label for="message">Message</label>
            <textarea id="message" name="message" placeholder="Write something..." style="height: 200px;" required></textarea>

            <div style="text-align: center;">
                <button type="submit" class="send-button">Send Message <i class="fa-solid fa-paper-plane"></i></button>
            </div>
        </form>
    </div>

    <!-- GDPR Notice -->
    <div class="container blog main">
        <div style="background: rgba(102, 126, 234, 0.05); padding: 20px; border-radius: 8px; margin-top: 20px; font-size: 0.6em; line-height: 1.3;">
            <h4 style="margin: 0 0 15px 0; color: #667eea; font-size: 0.85em;"><i class="fa-solid fa-shield-check" style="margin-right: 8px; color: #667eea;"></i>Data Protection Notice</h4>
            <p style="margin: 0 0 10px 0; color: #555; font-size: 0.65em;">
                <strong>What we collect:</strong> Your name, email address, and any information you provide in your message. 
                <strong>Why:</strong> To respond to your inquiry and keep you informed about the Safer Agentic AI Foundations research (with your consent). 
                <strong>Legal basis:</strong> Legitimate interest for inquiries, consent for research communications.
            </p>
            <p style="margin: 0 0 10px 0; color: #555; font-size: 0.65em;">
                <strong>Data sharing:</strong> We use Formspree to process submissions. Your data may be shared with research team members and academic collaborators as necessary for research coordination and collaboration.
            </p>
            <p style="margin: 0; color: #555; font-size: 0.65em;">
                <strong>Your rights:</strong> You may request access, correction, deletion, or portability of your data at any time. Contact us for data protection queries or to exercise your rights.
            </p>
        </div>
    </div>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p style="text-align: center;">
                This website content is based on "Safer Agentic AI Foundations, Volume 2" by The Agentic AI Safety Community of Practice.<br>
                Framework version I1, March 2025. — CC BY-ND 4.0<br>
                Website structure inspired by the <a href="https://shikun.io/projects/clarity" target="_blank" rel="noopener noreferrer">Clarity Template</a>, designed by <a href="https://shikun.io/" target="_blank" rel="noopener noreferrer">Shikun Liu</a>.
            </p>
        </div>    
    </footer>

</body>
</html>